<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false,"dimmer":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Why do we need batch normalization?We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize th">
<meta name="keywords" content="deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="BatchNormalization">
<meta property="og:url" content="http://yoursite.com/2019/04/02/BatchNormalization/index.html">
<meta property="og:site_name" content="钢蛋">
<meta property="og:description" content="Why do we need batch normalization?We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize th">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-04-03T07:31:22.696Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BatchNormalization">
<meta name="twitter:description" content="Why do we need batch normalization?We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize th">






  <link rel="canonical" href="http://yoursite.com/2019/04/02/BatchNormalization/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>BatchNormalization | 钢蛋</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">钢蛋</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/02/BatchNormalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="thylakoids">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="钢蛋">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">BatchNormalization

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-02 14:11:23" itemprop="dateCreated datePublished" datetime="2019-04-02T14:11:23+08:00">2019-04-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-03 15:31:22" itemprop="dateModified" datetime="2019-04-03T15:31:22+08:00">2019-04-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Why-do-we-need-batch-normalization"><a href="#Why-do-we-need-batch-normalization" class="headerlink" title="Why do we need batch normalization?"></a>Why do we need batch normalization?</h2><p>We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed.</p>
<p>Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift). To explain covariance shift, let’s have a deep network on cat detection. We train our data on only black cats’ images. So, if we now try to apply this network to data with colored cats, it is obvious; we’re not going to do well. The training set and the prediction set are both cats’ images but they differ a little bit. In other words, if an algorithm learned some X to Y mapping, and if the distribution of X changes, then we might need to retrain the learning algorithm by trying to align the distribution of X with the distribution of Y. ( Deeplearning.ai: Why Does Batch Norm Work?<a href="https://www.youtube.com/watch?v=nUUqwaxLnWs" target="_blank" rel="noopener">C2W3L06</a></p>
<p>Also, batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.</p>
<h2 id="How-dose-batch-normalization-work"><a href="#How-dose-batch-normalization-work" class="headerlink" title="How dose batch normalization work?"></a>How dose batch normalization work?</h2><p>Batch normalization layer is usually used <strong>before</strong> the activation layer.</p>
<p>The basic idea is doing the normalization then applying a linear and shif to the mini-batch:</p>
<p>For input mini-batch <script type="math/tex">B = \{x_{1, ..., m}\}</script>, we want to learn the parameter $\gamma$ and $\beta$. The output of the layer is <script type="math/tex">\{y_i = BN_{\gamma, \beta}(x_i)\}</script>, where:</p>
<script type="math/tex; mode=display">\mu_B \leftarrow \frac{1}{m}\sum_{i = 1}^{m}x_i</script><script type="math/tex; mode=display">\sigma_B^2 \leftarrow \frac{1}{m}\sum_{i=1}^m(x_i - \mu_B)^2</script><script type="math/tex; mode=display">\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}</script><script type="math/tex; mode=display">y_i \leftarrow \gamma \hat{x_i} + \beta \equiv BN_{\gamma,\beta}(x_i)</script><p><strong>Usual batchnorm</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t is the incoming tensor of shape [B, H, W, C]</span></span><br><span class="line"><span class="comment"># mean and stddev are computed along 0 axis and have shape [H, W, C]</span></span><br><span class="line">mean = mean(t, axis=<span class="number">0</span>)</span><br><span class="line">stddev = stddev(t, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0.</span>.B<span class="number">-1</span>:</span><br><span class="line">  out[i,:,:,:] = norm(t[i,:,:,:], mean, stddev)</span><br></pre></td></tr></table></figure></p>
<p>Ba/sically, it computes <code>H*W*C</code> means and <code>H*W*C</code> standard deviations across B<br>elements. You may notice that different elements at different spatial locations<br>have their own mean and variance and gather only <code>B</code> values.</p>
<p><strong>Batchnorm in conv layer</strong></p>
<p>This way is totally possible. But the convolutional layer has a special property: filter weights are shared across the input image (you can read it in detail in this <a href="http://cs231n.github.io/convolutional-networks/#conv" target="_blank" rel="noopener">post</a>. That’s why it’s reasonable to normalize the output in the same way, so that each output value takes the mean and variance of B<em>H</em>W values, at different locations.</p>
<p>Here’s how the code looks like in this case (again pseudo-code):<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t is still the incoming tensor of shape [B, H, W, C]</span></span><br><span class="line"><span class="comment"># but mean and stddev are computed along (0, 1, 2) axes and have just [C] shape</span></span><br><span class="line">mean = mean(t, axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">stddev = stddev(t, axis=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0.</span>.B<span class="number">-1</span>, x <span class="keyword">in</span> <span class="number">0.</span>.H<span class="number">-1</span>, y <span class="keyword">in</span> <span class="number">0.</span>.W<span class="number">-1</span>:</span><br><span class="line">  out[i,x,y,:] = norm(t[i,x,y,:], mean, stddev)</span><br></pre></td></tr></table></figure></p>
<p>In total, there are only <code>C</code> means and standard deviations and each one of them is computed over <code>B*H*W</code> values. That’s what they mean when they say “effective mini-batch”: the difference between the two is only in axis selection (or equivalently “mini-batch selection”).</p>
<h2 id="Batch-Normalization-with-backpropagation"><a href="#Batch-Normalization-with-backpropagation" class="headerlink" title="Batch Normalization with backpropagation"></a>Batch Normalization with backpropagation</h2><p> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">This link</a> provides a nice explanation of how to derive backpropagation gradients for batch normalization. Let’s see it in action now:</p>
<p> <strong>Batch Normalization for 2D data</strong></p>
<p> Let’s start with 1D normalization.<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure></p>
<p>Let’s define X and calculate its mean and variance. Note that X has size (20,100) which means it has 20 samples with each sample having 100 features (or dimensions). For normalizing, we need to look across all samples. In other words, we normalize across 20 values for each dimension (with each value coming on a sample).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">20</span>,<span class="number">100</span>) * <span class="number">5</span> + <span class="number">10</span></span><br><span class="line">X = Variable(X)</span><br><span class="line"></span><br><span class="line">mu = torch.mean(X[:,<span class="number">1</span>])</span><br><span class="line">var_ = torch.var(X[:,<span class="number">1</span>], unbiased=<span class="keyword">False</span>)</span><br><span class="line">sigma = torch.sqrt(var_ + <span class="number">1e-5</span>)</span><br><span class="line">x = (X[:,<span class="number">1</span>] - mu)/sigma</span><br></pre></td></tr></table></figure></p>
<p>Note that in the line above we set unbiased = False while calculating var_. This is to prevent Bessel’s correction. In other words, we want to divide by N and not N-1 while calculating the variance. x is the same as the result of batch normalization. Also note that in the code below we set affine = False. This is to prevent creation of parameters γ(k), β(k) which may scale and shift normalized data again. While training, affine is set to True.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">B = nn.BatchNorm1d(<span class="number">100</span>, affine=<span class="keyword">False</span>)</span><br><span class="line">y = B(X)</span><br><span class="line">print(x.data / y[:,<span class="number">1</span>].data)</span><br></pre></td></tr></table></figure></p>
<p>Output:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.0000</span><br><span class="line">1.0000</span><br><span class="line">1.0000</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>This also works for 3D input.</p>
<p><strong>Batch Normalization for 3D inputs</strong></p>
<p>Let’s define some 3D data with 4 and variance 4:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x3 = torch.randn(<span class="number">150</span>, <span class="number">20</span>, <span class="number">100</span>) * <span class="number">2</span> + <span class="number">4</span></span><br><span class="line">x3 = Variable(x3)</span><br><span class="line">B2 = nn.BatchNorm1d(<span class="number">20</span>)</span><br></pre></td></tr></table></figure></p>
<p>Note that here we did not set <code>affine = False</code>. Instead, we can manually set<br>those values to what we want. To preserve normalization, we want    γ(k) = 1 and<br>β(k) = 0. These values are stored in parameters weight and bias of the<br>BatchNorm variable.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">B2.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">B2.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">Y = B2(X3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Manual calculation</span></span><br><span class="line">mu = X3[:,<span class="number">0</span>,:].mean()</span><br><span class="line">sigma = torch.sqrt(torch.var(X3[:,<span class="number">0</span>,:], unbiased=<span class="keyword">False</span>) + <span class="number">1e-5</span>)</span><br><span class="line">X_normalized = (X3[:,<span class="number">0</span>,:] - mu)/sigma</span><br></pre></td></tr></table></figure></p>
<p>In the above example, X_normalized has the same values as Y[:,0,:].</p>
<p><strong>Batch Normalization for images(or any 4D input)</strong></p>
<p>A batch of RGB images has four dimensions: (B,C,X,Y) or (B,X,Y,C) where B is batch number, C is channel number, and X, Y are locations.</p>
<p>Batch normalization is images is done along the channel axis.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">5</span>,<span class="number">25</span>,<span class="number">100</span>,<span class="number">100</span>) * <span class="number">2</span> + <span class="number">4</span></span><br><span class="line">X = Variable(X)</span><br><span class="line">B = nn.BatchNorm2d(<span class="number">25</span>, affine=<span class="keyword">False</span>)</span><br><span class="line">Y = B(X)</span><br></pre></td></tr></table></figure></p>
<p>Here <code>Y[:,i,:,:]</code> is the same as <code>((X[:,i,:,:] - X[:,i,:,:].data.mean())/((X[:,i,:,:].data.var(unbiased=False) + 1e-5)**0.5))</code> for all valid values of.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" target="_blank" rel="noopener">Batch normalization in Neural Networks</a></p>
<p><a href="https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network" target="_blank" rel="noopener">Batch Normalization in Convolutional Neural Network</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/28/hexo/" rel="next" title="Hexo 建站指南">
                <i class="fa fa-chevron-left"></i> Hexo 建站指南
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/02/latex/" rel="prev" title="Latex Tutorial">
                Latex Tutorial <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">thylakoids</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-do-we-need-batch-normalization"><span class="nav-number">1.</span> <span class="nav-text">Why do we need batch normalization?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-dose-batch-normalization-work"><span class="nav-number">2.</span> <span class="nav-text">How dose batch normalization work?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization-with-backpropagation"><span class="nav-number">3.</span> <span class="nav-text">Batch Normalization with backpropagation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">4.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">thylakoids</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.0.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.0"></script>

  <script src="/js/src/motion.js?v=7.0.0"></script>



  
  


  <script src="/js/src/affix.js?v=7.0.0"></script>

  <script src="/js/src/schemes/pisces.js?v=7.0.0"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.0"></script>
<script src="/js/src/post-details.js?v=7.0.0"></script>



  


  <script src="/js/src/bootstrap.js?v=7.0.0"></script>


  
  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

</body>
</html>
